### 故障排查命令
```
journalctl -u kubelet
```
### 查看节点
```
kubectl get nodes
```

### 查看所有pod
```
kubectl get cs #查看组件
kubectl get pods -n kube-system
kubectl get pods --all-namespaces
kubectl get pods --namespace=kubernetes-dashboard
```

### 查看所有pods描述
```
kubectl describe pods kubernetes-dashboard-658b66597c-c9878 --namespace=kubernetes-dashboard
kubectl get po -n all-namespaces
docker search pod -n namespaces
```


删除并重新制作 pod
修复错误后，尝试通过删除并重新制作 pod 来修复 ImagePullBackOff 错误。使用以下命令：
kubectl delete pod
$ kubectl delete pod my-podpod "my-pod" deleted
这将删除 Pod 并使用更新的yaml文件再次创建它。如果镜像存在并且错误已修复，则 Pod 应该可以工作。
kubectl apply -f
$ kubectl apply -f my-pod.yamlpod/my-pod created

重启kubelet服务
```
systemctl restart kubelet
systemctl stop kubelet
```

重置k8s集群
需要在每台机器上执行
```
kubeadm reset
```
删除pod
```
kubectl delete pod calico-node-8w7vj --namespace=kube-system
```

重启主节点
```
setenforce 0
swapoff -a
iptables --flush
iptables -tnat --flush
systemctl stop firewalld
systemctl disable firewalld
sudo systemctl restart docker
service kubelet restart
export KUBECONFIG=/etc/kubernetes/admin.conf
```

重启从节点
```
setenforce 0
swapoff -a
iptables --flush
iptables -tnat --flush
systemctl stop firewalld
systemctl disable firewalld
sudo systemctl restart docker
service kubelet restart
export KUBECONFIG=/etc/kubernetes/admin.conf
```
重置 k8s 环境
```
sudo swapoff -a 
sudo kubeadm reset
sudo rm -rf /var/lib/cni/
sudo systemctl daemon-reload
```
回到 kubeadm init 的步骤


k8s拉取镜像失败处理 
拉取k8s网上的镜像失败后的状态ImagePullBackOff 
拉取本地的镜像失败后的状态ErrImageNeverPull
<!-- 使用 kubectl describe pod <pod-name> 命令查看 Pod 的状态和相关事件。 -->

拉取//合适//的镜像到本地
```
docker pull lbbi/prometheus-adapter:v0.9.1
```
给下载下来的镜像打标签。可以选择不打标签，但是yaml配置文件修改时要写对镜像标签，否则就会出现找不到本地镜像的报错，pod 状态为 “ ErrImageNeverPull ”
docker images    ## 此处为已经打好标签的镜像。
打标签命令
```
[root@k8s-master01 manifests]# docker tag docker.io/lbbi/kube-state-metrics:v2.4.1 k8s.gcr.io/kube-state-metrics:v2.4.1
```

```
k8s集群内的所以设备都需要拉取一样的镜像，并且打上一样的标签，因为k8s在没有做亲和力的情况下，部署pod是在集群内随机部署的。若没有全部拉取镜像，也会出现找不到本地镜像的问题，pod状态是 “ErrImageNeverPull”。
```

为了方便k8s找到本地的镜像文件，需要修改pod对应的yaml配置文件。
找到需要镜像的yaml文件
```
[root@k8s-master01 manifests]# vim prometheusAdapter-deployment.yaml
```
***  找到拉取镜像失败的部分  “修改前”  ***
```
image: k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1
name: prometheus-adapter
```
*** 修改后***
```
image: k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1
image: k8s.gcr.io/prometheus-adapter:v0.9.1   #  容器镜像标签，写错拉取本地镜像失败。
imagePullPolicy: Never                       
imagePullPolicy: Always 总是网络拉取镜像, 是k8s默认的拉取方式。
imagePullPolicy: Never 从不远程拉取镜像，只读取本地镜像。
imagePullPolicy: IfNotPresent 优先拉取本地镜像。
name: prometheus-adapter 
ports:
- containerPort: 6443
```
重新加载pod 的yaml 文件，并查询pod状态。



主节点重启后报错
```
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```
原因分析
```
kubernetes master没有与本机绑定，集群初始化的时候没有绑定，导致kubectl 无法识别master是自身。
```
编辑文件设置
```
vim /etc/profile
在底部增加新的环境变量 export KUBECONFIG=/etc/kubernetes/admin.conf
```
使生效​
```
source /etc/profile
```

kubernetes和docker cgroup驱动程序是否设置为相同
对于docker添加到/etc/docker/daemon.json中
```
docker info |grep -i cgroup
```

```
{
    "exec-opts": ["native.cgroupdriver=systemd"]
}
```
重新启动docker服务
```
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo systemctl restart kubelet
```
尝试执行kubeadm join
